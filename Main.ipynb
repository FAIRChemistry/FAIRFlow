{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Workflow for on-line GC and HPLC analysis in flow chemistry</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Imports, Paths, and Logging\n",
    "---\n",
    "\n",
    "In this section all the necessary python packages are imported, the path to this notebook and the logger for this notebook is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate autoreload to keep on track with changing modules #\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import standard libraries #\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import tools for data processing and analysis and DaRUS upload #\n",
    "from datamodel.tools import initialize_dataset, reading_raw_data_widget, analyzing_raw_data_widget, DaRUS_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for loggin output #\n",
    "root                = Path.cwd()\n",
    "logging_config_path = root / \"datamodel/tools/logger_config.json\"\n",
    "\n",
    "# Read in logger specs and configurate logger (set name to current notebook) #\n",
    "with open(logging_config_path) as logging_config_json: logging.config.dictConfig( json.load( logging_config_json ) )\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the level of thid-party logger to avoid dumping too much information #\n",
    "for logger_ in ['markdown_it', 'h5py', 'numexpr', 'git']: logging.getLogger(logger_).setLevel('WARNING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Dataset and raw data\n",
    "---\n",
    "In this section the dataset as well as the to analyze raw data is choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100854f0e85048b582c226b19f768dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Choose datamodel', layout=Layout(width='auto'), options=((…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset!\n"
     ]
    }
   ],
   "source": [
    "git_path = 'https://github.com/FAIRChemistry/datamodel_b07_tc.git'\n",
    "branch   = 'samir_develop'\n",
    "\n",
    "id = initialize_dataset()\n",
    "id.write_dataset(root, git_path, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of basic meta data of the project ##\n",
    "\n",
    "id.title.value        = 'Electrocatalytic CO2-reduction on carbon'\n",
    "id.description.value  = 'The aim of this project is to blablabla'\n",
    "id.project.value      = 'Project B07'\n",
    "\n",
    "# List with authors and their affiliation #\n",
    "id.authors.value      = 'Richard Schömig, Maximilian Schmidt' \n",
    "id.affiliations.value = 'University of Stuttgart, University of Stuttgart'\n",
    "id.identifier.value   = 'xxx-xxx-xxx-xxx, xxx-xxx-xxx-xxx'\n",
    "id.contact_text.value = 'Richard Schömig, richard@web.de'\n",
    "\n",
    "id.related_publication.value = \"test, https://id.loc.gov/authorities/subjects/sh2014001146.html\"\n",
    "\n",
    "id.topic_classification.value = \"homogeneous catalysis (LCSH), https://id.loc.gov/authorities/subjects/sh2014001146.html\"\n",
    "id.keywords.value             = \"polymer chemistry (Loterre Chemistry Vocabulary), https://skosmos.loterre.fr/ERC/en/page/?uri=http%3A%2F%2Fdata.loterre.fr%2Fark%3A%2F67375%2FERC-KCSKD4X9-P\"\n",
    "\n",
    "id.dataset_text.value = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874872f097ea484ab8b4892b61c3c415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Choose dataset', layout=Layout(width='auto'), options=(('b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Search for dataset and raw data ##\n",
    "\n",
    "rrdw = reading_raw_data_widget()\n",
    "rrdw.choose_data(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(root) = \"c:/Users/darouich/OneDrive/Dokumente/\"\n",
    "\n",
    "e_chem = str(root)+'/data/Rohdaten/01_EChem/CAD14-Cu@AB/GSTATIC.DTA'\n",
    "mfm    = str(root)+'/data/Rohdaten/03_MFM/CAD14-Cu@AB/Bench-2h-GSS_CAD14-Cu@AB_200_50c_24h_truncated.csv'\n",
    "gc     = [str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0102.D/report00.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0102.D/REPORT01.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0103.D/report00.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0103.D/REPORT01.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0104.D/report00.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0104.D/REPORT01.CSV']\n",
    "calib  = str(root)+'/data/calibration/calibration.json'\n",
    "correc = str(root)+'/data/correction_factors/correction_factors.json'\n",
    "farada = str(root)+'/data/faraday_coefficients/faraday_coefficients.json'\n",
    "\n",
    "rrdw.Echem_files.value = [e_chem]\n",
    "rrdw.MFM_files.value   = [mfm]\n",
    "rrdw.GC_files.value    = gc\n",
    "rrdw.calib_files.value = [calib]\n",
    "rrdw.correction_files.value = [correc]\n",
    "rrdw.faraday_files.value    = [farada]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Analysis of data\n",
    "---\n",
    "In this section the raw data of the above choosen dataset is analyzed (if you change the dataset above, then reexecute this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b73e9342e54a1f83f420b48be09777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Choose experiment:', layout=Layout(width='auto'), options=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376fbbc8a70c4f26bca8e83fba4953d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Label(value='Measurement number 0', layout=Layout(height='30px', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723f64a461064f13bade13ff4ab9c3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='The mass flow at the time of the GC measurement is determined by mat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Provide a typical retention time dictionary to pre assign retention times \n",
    "\n",
    "typical_retention_time = {\"Hydrogen\": 1.7, \"Carbon dioxide\": 3.0, \"Carbon monoxide\": 13.6, \n",
    "                          \"Methane\": 3.6, \"Ethene\": 6.0, \"Ethane\": 7.1}\n",
    "\n",
    "ardw = analyzing_raw_data_widget()\n",
    "ardw.choose_experiment( datamodel = rrdw.datamodel, \n",
    "                        dataset_path = rrdw.dataset_dropdown.value, \n",
    "                        typical_retention_time = typical_retention_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Upload of data to DaRUS\n",
    "---\n",
    "In this section the dataset containing the processed as well as the raw data, is uploaded to DaRUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e73e529be444458d3098f919ae12d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Choose whether you want to create a new data record or edit an existing o…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute project not valid for import (dv_up).\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Validation Failed: Depositor is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]).java.util.stream.ReferencePipeline$3@6126ae76",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\darouich\\OneDrive\\Dokumente\\datamodel_b07_tc\\datamodel_b07_tc\\datamodel\\tools\\data_processing_widgets.py:661\u001b[0m, in \u001b[0;36mupload_to_DaRUS\u001b[1;34m(self, _)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe specified entry is neither a file nor a directory:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mentry\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[39m## Upload ##\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDaRUS_data\u001b[39m.\u001b[39mupload( dataverse_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataverse_dropdown\u001b[39m.\u001b[39mvalue,\n\u001b[1;32m--> 661\u001b[0m                    DATAVERSE_URL  \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://darus.uni-stuttgart.de\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    662\u001b[0m                    API_TOKEN      \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_token_text\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\easyDataverse\\core\\dataset.py:267\u001b[0m, in \u001b[0;36mDataset.upload\u001b[1;34m(self, dataverse_name, content_loc, DATAVERSE_URL, API_TOKEN)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupload\u001b[39m(\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    250\u001b[0m     dataverse_name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m     API_TOKEN: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    254\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    255\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uploads a given dataset to a Dataverse installation specified in the environment variable.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \n\u001b[0;32m    257\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m        str: [description]\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_id \u001b[39m=\u001b[39m upload_to_dataverse(\n\u001b[0;32m    268\u001b[0m         json_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataverse_json(),\n\u001b[0;32m    269\u001b[0m         dataverse_name\u001b[39m=\u001b[39;49mdataverse_name,\n\u001b[0;32m    270\u001b[0m         files\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    271\u001b[0m         p_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_id,\n\u001b[0;32m    272\u001b[0m         DATAVERSE_URL\u001b[39m=\u001b[39;49mDATAVERSE_URL,\n\u001b[0;32m    273\u001b[0m         API_TOKEN\u001b[39m=\u001b[39;49mAPI_TOKEN,\n\u001b[0;32m    274\u001b[0m         content_loc\u001b[39m=\u001b[39;49mcontent_loc,\n\u001b[0;32m    275\u001b[0m     )\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_id\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\easyDataverse\\tools\\uploader\\uploader.py:62\u001b[0m, in \u001b[0;36mupload_to_dataverse\u001b[1;34m(json_data, dataverse_name, files, p_id, content_loc, DATAVERSE_URL, API_TOKEN)\u001b[0m\n\u001b[0;32m     59\u001b[0m     response \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mcreate_dataset(dataverse_name, json_data)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOK\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     64\u001b[0m \u001b[39m# Get response data\u001b[39;00m\n\u001b[0;32m     65\u001b[0m p_id \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mpersistentId\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mException\u001b[0m: Validation Failed: Depositor is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]).java.util.stream.ReferencePipeline$3@6126ae76"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DaRUS_upload.update_to_DaRUS() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: DaRUS_upload.update_to_DaRUS() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "sfb1333_dataverse_list = [ \"sfb1333-hansen-gross\" ]\n",
    "\n",
    "api_token = \"4afecd82-c92d-4935-b786-2225af43531e\"\n",
    "\n",
    "test = DaRUS_upload()\n",
    "\n",
    "test.upload( datamodel = rrdw.datamodel, \n",
    "             dataset_path = rrdw.dataset_dropdown.value,\n",
    "             dataverse_list = sfb1333_dataverse_list )\n",
    "\n",
    "#test.depositor_text.value = \"samir Darouich\"\n",
    "test.api_token_text.value = api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiUrlError",
     "evalue": "base_url None is not a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiUrlError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\darouich\\OneDrive\\Dokumente\\datamodel_b07_tc\\datamodel_b07_tc\\Main.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/darouich/OneDrive/Dokumente/datamodel_b07_tc/datamodel_b07_tc/Main.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Dataset\u001b[39m.\u001b[39;49mfrom_dataverse_doi(\u001b[39m'\u001b[39;49m\u001b[39mdoi:10.18419/darus-3812\u001b[39;49m\u001b[39m'\u001b[39;49m, filedir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataverse_url\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhttps://darus.uni-stuttgart.de\u001b[39;49m\u001b[39m'\u001b[39;49m,api_token\u001b[39m=\u001b[39;49mapi_token)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\pydantic\\decorator.py:40\u001b[0m, in \u001b[0;36mvalidate_arguments.<locals>.validate.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m@wraps\u001b[39m(_func)\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper_function\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m vd\u001b[39m.\u001b[39;49mcall(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\pydantic\\decorator.py:134\u001b[0m, in \u001b[0;36mValidatedFunction.call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    133\u001b[0m     m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_model_instance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(m)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\pydantic\\decorator.py:201\u001b[0m, in \u001b[0;36mValidatedFunction.execute\u001b[1;34m(self, m)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_function(\u001b[39m*\u001b[39margs_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvar_kwargs)\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_function(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49md, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvar_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\easyDataverse\\core\\dataset.py:415\u001b[0m, in \u001b[0;36mDataset.from_dataverse_doi\u001b[1;34m(cls, doi, filedir, filenames, download_files, lib_name, dataverse_url, api_token)\u001b[0m\n\u001b[0;32m    413\u001b[0m     dataverse_url, api_token \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fetch_env_vars()\n\u001b[0;32m    414\u001b[0m     lib_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mEASYDATAVERSE_LIB_NAME\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_with_lib(\n\u001b[0;32m    416\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m(),\n\u001b[0;32m    417\u001b[0m         doi\u001b[39m=\u001b[39;49mdoi,\n\u001b[0;32m    418\u001b[0m         lib_name\u001b[39m=\u001b[39;49mlib_name,\n\u001b[0;32m    419\u001b[0m         filedir\u001b[39m=\u001b[39;49mfiledir,\n\u001b[0;32m    420\u001b[0m         dataverse_url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    421\u001b[0m         api_token\u001b[39m=\u001b[39;49mapi_token,\n\u001b[0;32m    422\u001b[0m         filenames\u001b[39m=\u001b[39;49mfilenames,\n\u001b[0;32m    423\u001b[0m     )\n\u001b[0;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m     dataverse_url, api_token \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fetch_env_vars()\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\easyDataverse\\core\\dataset.py:459\u001b[0m, in \u001b[0;36mDataset._fetch_with_lib\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_with_lib\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    458\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fetches the dataset with a dedicated library.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m download_from_dataverse_with_lib(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\easyDataverse\\tools\\downloader\\downloader.py:35\u001b[0m, in \u001b[0;36mdownload_from_dataverse_with_lib\u001b[1;34m(dataset, doi, lib_name, filedir, filenames, dataverse_url, api_token)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Downloads a dataset from a Dataverse instance and initializes a Dataset object.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m    api_token (str): API Token used to authorize at the dataverse installation. Can be inferred from env vars.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# Intialize the pyDataverse instance to fetch the dataset\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m api \u001b[39m=\u001b[39m NativeApi(dataverse_url, api_token)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Download the dataset and retrieve the field data\u001b[39;00m\n\u001b[0;32m     38\u001b[0m dv_dataset \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mget_dataset(doi)\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\pyDataverse\\api.py:639\u001b[0m, in \u001b[0;36mNativeApi.__init__\u001b[1;34m(self, base_url, api_token, api_version)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, base_url: \u001b[39mstr\u001b[39m, api_token\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, api_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mv1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    628\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Init an Api() class.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[0;32m    630\u001b[0m \u001b[39m    Scheme, host and path combined create the base-url for the api.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \n\u001b[0;32m    638\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(base_url, api_token, api_version)\n\u001b[0;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url_api_native \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url_api\n",
      "File \u001b[1;32mc:\\Users\\darouich\\miniconda3\\Lib\\site-packages\\pyDataverse\\api.py:61\u001b[0m, in \u001b[0;36mApi.__init__\u001b[1;34m(self, base_url, api_token, api_version)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Init an Api() class.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39mScheme, host and path combined create the base-url for the api.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(base_url, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mraise\u001b[39;00m ApiUrlError(\u001b[39m\"\u001b[39m\u001b[39mbase_url \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m is not a string.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(base_url))\n\u001b[0;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url \u001b[39m=\u001b[39m base_url\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(api_version, (\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)):\n",
      "\u001b[1;31mApiUrlError\u001b[0m: base_url None is not a string."
     ]
    }
   ],
   "source": [
    "Dataset.from_dataverse_doi('doi:10.18419/darus-3812', filedir=\".\", dataverse_url='https://darus.uni-stuttgart.de', api_token=api_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f62ab2d43dc75e3c3b007469adeb0f7488873df876b9b71dd3b119f0280ba41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
