{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Workflow for on-line GC and HPLC analysis in flow chemistry</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Imports, Paths, and Logging\n",
    "---\n",
    "\n",
    "In this section all the necessary python packages are imported, the path to this notebook and the logger for this notebook is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate autoreload to keep on track with changing modules #\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import standard libraries #\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import tools for data processing and analysis #\n",
    "from datamodel.tools import initialize_dataset\n",
    "from datamodel.tools import reading_raw_data_widget\n",
    "from datamodel.tools import analyzing_raw_data_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for loggin output #\n",
    "root                = Path.cwd()\n",
    "logging_config_path = root / \"datamodel_b07_tc/tools/logging/config.json\"\n",
    "\n",
    "# Read in logger specs and configurate logger (set name to current notebook) #\n",
    "with open(logging_config_path) as logging_config_json: logging.config.dictConfig( json.load( logging_config_json ) )\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the level of thid-party logger to avoid dumping too much information #\n",
    "for logger_ in ['markdown_it', 'h5py', 'numexpr', 'git']: logging.getLogger(logger_).setLevel('WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fragen an Jan:\n",
    "\n",
    "# Wenn ich dataset wieder einlese, ist das format nicht mehr das was es war davor --> kann ich es wiederherstellen (float, datetime etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Dataset and raw data\n",
    "---\n",
    "In this section the dataset as well as the to analyze raw data is choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d261b20a404ce8ae99fbc9dbe40716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Choose datamodel', layout=Layout(width='auto'), options=(('git', 'https:/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset!\n"
     ]
    }
   ],
   "source": [
    "git_path = 'https://github.com/FAIRChemistry/datamodel_b07_tc.git'\n",
    "branch   = 'samir_develop'\n",
    "\n",
    "id = initialize_dataset()\n",
    "id.write_dataset(root, git_path, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of basic meta data of the project ##\n",
    "\n",
    "id.title.value        = 'Electrocatalytic CO2-reduction on carbon'\n",
    "id.description.value  = 'The aim of this project is to blablabla'\n",
    "\n",
    "# List with authors and their affiliation #\n",
    "id.authors.value      = 'Richard Schömig, Maximilian Schmidt' \n",
    "id.affiliations.value = 'University of Stuttgart, University of Stuttgart'\n",
    "id.dataset_text.value = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3324b657f51b4e73a22264e376060bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Choose dataset', layout=Layout(width='auto'), options=(('b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Search for dataset and raw data ##\n",
    "\n",
    "rrdw = reading_raw_data_widget()\n",
    "rrdw.choose_data(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(root) = \"c:/Users/darouich/OneDrive/Dokumente/\"\n",
    "\n",
    "e_chem = str(root)+'/data/Rohdaten/01_EChem/CAD14-Cu@AB/GSTATIC.DTA'\n",
    "mfm    = str(root)+'/data/Rohdaten/03_MFM/CAD14-Cu@AB/Bench-2h-GSS_CAD14-Cu@AB_200_50c_24h_truncated.csv'\n",
    "gc     = [str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0102.D/report00.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0102.D/REPORT01.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0103.D/report00.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0103.D/REPORT01.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0104.D/report00.CSV',\n",
    "        str(root)+'/data/Rohdaten/02_GC/CAD14-Cu@AB/JH-1H 2023-02-06 10-00-18/NV-F0104.D/REPORT01.CSV']\n",
    "calib  = str(root)+'/data/calibration/calibration.json'\n",
    "correc = str(root)+'/data/correction_factors/correction_factors.json'\n",
    "farada = str(root)+'/data/faraday_coefficients/faraday_coefficients.json'\n",
    "\n",
    "rrdw.Echem_files.value = [e_chem]\n",
    "rrdw.MFM_files.value   = [mfm]\n",
    "rrdw.GC_files.value    = gc\n",
    "rrdw.calib_files.value = [calib]\n",
    "rrdw.correction_files.value = [correc]\n",
    "rrdw.faraday_files.value    = [farada]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Analysis of data\n",
    "---\n",
    "In this section the raw data of the above choosen dataset is analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281d616002df42869e71aba814befaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Choose experiment:', layout=Layout(width='auto'), options=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f378bb4d0b04ed0af1f6a52573011fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Label(value='Measurement number 0', layout=Layout(height='30px', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a230af5f13c458090cc564bed4dce51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(VBox(children=(HTML(value='The mass flow at the time of the GC measurement is de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting the postprocessing\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/PhD/SFB1333/datamodel_b07_tc/datamodel/tools/data_processing_widgets.py:397\u001b[0m, in \u001b[0;36manalyzing_raw_data_widget.do_postprocessing\u001b[0;34m(self, _)\u001b[0m\n\u001b[1;32m    394\u001b[0m gc_measurements      \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mexperiments[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiments_dropdown\u001b[39m.\u001b[39mvalue]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmeasurements\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmeasurement_type\u001b[39m\u001b[39m\"\u001b[39m, MeasurementType\u001b[39m.\u001b[39mGC\u001b[39m.\u001b[39mvalue)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    396\u001b[0m \u001b[39mfor\u001b[39;00m i, gc_measurement \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m( gc_measurements ):\n\u001b[0;32m--> 397\u001b[0m     tmp \u001b[39m=\u001b[39m fe_calculator\u001b[39m.\u001b[39;49mcalculate_faraday_efficiencies( gc_measurement \u001b[39m=\u001b[39;49m gc_measurement )\n\u001b[1;32m    398\u001b[0m     faraday_efficiencies\u001b[39m.\u001b[39mappend( tmp )\n\u001b[1;32m    399\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFaraday effiencies of GC measurement n°\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39mi)\n",
      "File \u001b[0;32m~/Documents/PhD/SFB1333/datamodel_b07_tc/datamodel/tools/calculator.py:138\u001b[0m, in \u001b[0;36mFaradayEfficiencyCalculator.calculate_faraday_efficiencies\u001b[0;34m(self, gc_measurement)\u001b[0m\n\u001b[1;32m    135\u001b[0m         assigned_peak_areas_dict[species] \u001b[39m=\u001b[39m [peak_area]\n\u001b[1;32m    137\u001b[0m \u001b[39m# Compute the theoretical material flow (several substeps are performed to do so)\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calculate_volumetric_flow_mean( inj_date_datetime \u001b[39m=\u001b[39;49m inj_date_datetime )\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_volumetric_fractions( assigned_peak_areas_dict \u001b[39m=\u001b[39m assigned_peak_areas_dict)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_real_volumetric_flow()\n",
      "File \u001b[0;32m~/Documents/PhD/SFB1333/datamodel_b07_tc/datamodel/tools/calculator.py:45\u001b[0m, in \u001b[0;36mFaradayEfficiencyCalculator._calculate_volumetric_flow_mean\u001b[0;34m(self, inj_date_datetime)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mFunction checks the GC injection time and search the corresponding time in the mass flow meter recordings.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mThe times are matched and the mean mass flow is computed as an average over the x steps bevore and after\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m    float: Averaged volumetric flow [ml/s]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m volumetric_flow_datetime_list, volumetric_flow_values_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39mvolumetric_flow_time_course\n\u001b[0;32m---> 45\u001b[0m index_match                \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin( [\u001b[39mabs\u001b[39;49m(dt \u001b[39m-\u001b[39;49m inj_date_datetime) \u001b[39mfor\u001b[39;49;00m dt \u001b[39min\u001b[39;49;00m volumetric_flow_datetime_list] )\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvolumetric_flow_mean  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean( np\u001b[39m.\u001b[39marray( volumetric_flow_values_list )[ \u001b[39mrange\u001b[39m(index_match \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_radius, index_match \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_radius \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) ] )\n",
      "File \u001b[0;32m~/Documents/PhD/SFB1333/datamodel_b07_tc/datamodel/tools/calculator.py:45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mFunction checks the GC injection time and search the corresponding time in the mass flow meter recordings.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mThe times are matched and the mean mass flow is computed as an average over the x steps bevore and after\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m    float: Averaged volumetric flow [ml/s]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m volumetric_flow_datetime_list, volumetric_flow_values_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39mvolumetric_flow_time_course\n\u001b[0;32m---> 45\u001b[0m index_match                \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin( [\u001b[39mabs\u001b[39m(dt \u001b[39m-\u001b[39;49m inj_date_datetime) \u001b[39mfor\u001b[39;00m dt \u001b[39min\u001b[39;00m volumetric_flow_datetime_list] )\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvolumetric_flow_mean  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean( np\u001b[39m.\u001b[39marray( volumetric_flow_values_list )[ \u001b[39mrange\u001b[39m(index_match \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_radius, index_match \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_radius \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) ] )\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Provide a typical retention time dictionary to pre assign retention times \n",
    "\n",
    "typical_retention_time = {\"Hydrogen\": 1.7, \"Carbon dioxide\": 3.0, \"Carbon monoxide\": 13.6, \n",
    "                          \"Methane\": 3.6, \"Ethene\": 6.0, \"Ethane\": 7.1}\n",
    "\n",
    "ardw = analyzing_raw_data_widget()\n",
    "ardw.choose_experiment( datamodel = rrdw.datamodel, \n",
    "                        dataset_path = rrdw.dataset_dropdown.value, \n",
    "                        typical_retention_time = typical_retention_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yers\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datamodel.core import Dataset, Experiment\n",
    "import pandas as pd\n",
    "dataset, lib = rrdw.datamodel\n",
    "c = Experiment( **rrdw.dataset.experiments[0].__dict__ )\n",
    "\n",
    "a,b = c.volumetric_flow_time_course\n",
    "#--> implement that when rereading the dataset the correct data types e.g float, datetime, etc are restorted !!!!\n",
    "\n",
    "d = [ pd.to_datetime(timestamp, format=\"%Y-%m-%d %H:%M:%S\").to_pydatetime()\n",
    "                   for timestamp in a ]\n",
    "if type(d[0]) == datetime:\n",
    "    print(\"yers\")\n",
    "\n",
    "type( Dataset(**dataset.__dict__).experiments[0].measurements[2].metadata[0].value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Upload of data to DaRUS\n",
    "---\n",
    "In this section the processed, as well as the raw data is uploaded to DaRUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute project not valid for import (dv_up).\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Validation Failed: Point of Contact Name is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Description Text is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Project Level is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Funding Information Identifier is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Project Name is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Deposit Date is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Author Name is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Depositor is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Point of Contact E-mail is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Funding Information Agency is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Title is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Subject is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]).java.util.stream.ReferencePipeline$3@567ca205",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/samir/Documents/PhD/SFB1333/datamodel_b07_tc/Main.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samir/Documents/PhD/SFB1333/datamodel_b07_tc/Main.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m dataset\u001b[39m.\u001b[39madd_file(dv_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, local_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy.file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samir/Documents/PhD/SFB1333/datamodel_b07_tc/Main.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#https://darus.uni-stuttgart.de/api/dataverses/sfb1333\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samir/Documents/PhD/SFB1333/datamodel_b07_tc/Main.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m dataset\u001b[39m.\u001b[39;49mupload(dataverse_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msfb1333-hansen-gross\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samir/Documents/PhD/SFB1333/datamodel_b07_tc/Main.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                DATAVERSE_URL\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://darus.uni-stuttgart.de\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samir/Documents/PhD/SFB1333/datamodel_b07_tc/Main.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                API_TOKEN\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m4afecd82-c92d-4935-b786-2225af43531e\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/easyDataverse/core/dataset.py:267\u001b[0m, in \u001b[0;36mDataset.upload\u001b[0;34m(self, dataverse_name, content_loc, DATAVERSE_URL, API_TOKEN)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupload\u001b[39m(\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    250\u001b[0m     dataverse_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     API_TOKEN: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    255\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uploads a given dataset to a Dataverse installation specified in the environment variable.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[1;32m    257\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m        str: [description]\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_id \u001b[39m=\u001b[39m upload_to_dataverse(\n\u001b[1;32m    268\u001b[0m         json_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataverse_json(),\n\u001b[1;32m    269\u001b[0m         dataverse_name\u001b[39m=\u001b[39;49mdataverse_name,\n\u001b[1;32m    270\u001b[0m         files\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfiles,\n\u001b[1;32m    271\u001b[0m         p_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_id,\n\u001b[1;32m    272\u001b[0m         DATAVERSE_URL\u001b[39m=\u001b[39;49mDATAVERSE_URL,\n\u001b[1;32m    273\u001b[0m         API_TOKEN\u001b[39m=\u001b[39;49mAPI_TOKEN,\n\u001b[1;32m    274\u001b[0m         content_loc\u001b[39m=\u001b[39;49mcontent_loc,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_id\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/easyDataverse/tools/uploader/uploader.py:62\u001b[0m, in \u001b[0;36mupload_to_dataverse\u001b[0;34m(json_data, dataverse_name, files, p_id, content_loc, DATAVERSE_URL, API_TOKEN)\u001b[0m\n\u001b[1;32m     59\u001b[0m     response \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mcreate_dataset(dataverse_name, json_data)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOK\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     64\u001b[0m \u001b[39m# Get response data\u001b[39;00m\n\u001b[1;32m     65\u001b[0m p_id \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mpersistentId\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: Validation Failed: Point of Contact Name is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Description Text is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Project Level is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Funding Information Identifier is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Project Name is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Deposit Date is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Author Name is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Depositor is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Point of Contact E-mail is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Funding Information Agency is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Title is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]), Subject is required. (Invalid value:edu.harvard.iq.dataverse.DatasetField[ id=null ]).java.util.stream.ReferencePipeline$3@567ca205"
     ]
    }
   ],
   "source": [
    "from pyDaRUS import Citation\n",
    "from pyDaRUS import Dataset\n",
    "from pyDaRUS.metadatablocks.citation import SubjectEnum, IdentifierScheme\n",
    "\n",
    "subject_list = [ subject.value for subject in SubjectEnum]\n",
    "\n",
    "# Initialize the metadatablock\n",
    "citation = Citation()\n",
    "\n",
    "citation.title = \"My Title\"\n",
    "\n",
    "citation.add_description(text=\"test\")\n",
    "citation.add_author(name=\"Samir\", affiliation=\"ITT\")\n",
    "citation.add_contact(name=\"Samir\", email=\"samir.darouich@itt.uni-stuttgart.de\")\n",
    "\n",
    "citation.subject = [\"Engineering\"]\n",
    "\n",
    "citation.add_grant_information( grant_agency=\"DFG\", grant_number=\"358283783 - SFB 1333\")\n",
    "citation.add_project( name=\"test project\", level=1 )\n",
    "citation.depositor = \"XXX\"\n",
    "citation.deposit_date = \"XXX\"\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.add_metadatablock(citation)\n",
    "\n",
    "dataset.add_file(dv_path=\".\", local_path=\"my.file\")\n",
    "\n",
    "dataset.upload(dataverse_name=\"sfb1333-hansen-gross\",\n",
    "               DATAVERSE_URL=\"https://darus.uni-stuttgart.de\",\n",
    "               API_TOKEN=\"4afecd82-c92d-4935-b786-2225af43531e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f62ab2d43dc75e3c3b007469adeb0f7488873df876b9b71dd3b119f0280ba41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
